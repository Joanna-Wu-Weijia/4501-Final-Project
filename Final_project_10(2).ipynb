{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import math\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "import sqlite3\n",
    "import sqlalchemy as db\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import folium\n",
    "from folium.plugins import HeatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TLC_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "WEATHER_CSV_DIR = \"\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones():\n",
    "    \"\"\"\n",
    "    Load and transform NYC taxi zone shapefile data to WGS84 coordinates.\n",
    "\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: GeoDataFrame containing taxi zone data with columns:\n",
    "            - LocationID: unique identifier for each zone\n",
    "            - zone: name of the taxi zone\n",
    "            - borough: borough the zone is in\n",
    "            - geometry: geometric boundaries of the zone\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the taxi zones shapefile cannot be loaded\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # use WGS84 cord\n",
    "        taxi_zones = gpd.read_file('taxi_zones.shp').to_crs(CRS)\n",
    "        return taxi_zones[['LocationID', 'zone', 'borough', 'geometry']]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading shapefile: {e}\")\n",
    "        raise ValueError(\"Could not load taxi zones shapefile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d04c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(\n",
    "    zone_loc_id: int, \n",
    "    loaded_taxi_zones: gpd.GeoDataFrame\n",
    "):\n",
    "    r\"\"\"\n",
    "    Look up centroid coordinates for a given taxi zone ID.\n",
    "\n",
    "    Args:\n",
    "        zone_loc_id: LocationID of the taxi zone to find coordinates for\n",
    "        loaded_taxi_zones: GeoDataFrame containing taxi zone data\n",
    "\n",
    "    Returns:\n",
    "        tuple: (latitude, longitude) coordinates of zone centroid if found\n",
    "        None: if zone lookup fails\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If no taxi zone found for the given LocationID\n",
    "    \"\"\"\n",
    "    try:\n",
    "        zone = loaded_taxi_zones[loaded_taxi_zones['LocationID'] == zone_loc_id]\n",
    "        if zone.empty:\n",
    "            raise ValueError(f\"No taxi zone found for LocationID: {zone_loc_id}\")\n",
    "        \n",
    "        centroid = zone.geometry.centroid.iloc[0]\n",
    "        return (centroid.y, centroid.x)\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"Value error: {e}\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error looking up coordinates: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_size(\n",
    "   population: int,\n",
    "   confidence_level: float = 0.95,\n",
    "   margin_of_error: float = 0.05,\n",
    "   p: float = 0.5\n",
    ") -> int:\n",
    "   r\"\"\"\n",
    "   Calculate required sample size using Cochran's formula.\n",
    "\n",
    "   Args:\n",
    "       population: Total size of the population to sample from\n",
    "       confidence_level: Desired confidence level (default 0.95)\n",
    "       margin_of_error: Acceptable margin of error (default 0.05)\n",
    "       p: Population proportion (default 0.5)\n",
    "\n",
    "   Returns:\n",
    "       int: Required sample size, rounded up to nearest integer\n",
    "\n",
    "   Notes:\n",
    "       Uses Cochran's formula for calculating sample size:\n",
    "       n = (z^2 * p * (1-p)) / e^2\n",
    "       where:\n",
    "       z = z-score for confidence level\n",
    "       e = margin of error\n",
    "       p = population proportion\n",
    "   \"\"\"\n",
    "   # Z-score lookup for common confidence levels\n",
    "   z = {0.90: 1.645, 0.95: 1.96, 0.99: 2.576}[confidence_level]\n",
    "   e = margin_of_error\n",
    "\n",
    "   # Calculate base sample size\n",
    "   numerator = (z**2) * p * (1 - p)\n",
    "   denominator = e**2\n",
    "   sample_size = numerator / denominator\n",
    "\n",
    "   # Adjust for finite population if provided\n",
    "   if population:\n",
    "       sample_size = (sample_size * population) / (sample_size + population - 1)\n",
    "\n",
    "   return math.ceil(sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33eed4-b2e9-4ab3-94a8-59f04e464c98",
   "metadata": {},
   "source": [
    "### Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_tlc_page(tlc_url: str) -> list[str]:\n",
    "   r\"\"\"\n",
    "   Extract all URLs from the TLC (Taxi & Limousine Commission) webpage.\n",
    "\n",
    "   Args:\n",
    "       tlc_url: URL of the TLC webpage to scrape\n",
    "\n",
    "   Returns:\n",
    "       list[str]: List of all URLs found on the page\n",
    "\n",
    "   Raises:\n",
    "       RequestException: If webpage cannot be accessed\n",
    "       ParserError: If HTML parsing fails\n",
    "   \"\"\"\n",
    "   response = requests.get(tlc_url)\n",
    "   soup = BeautifulSoup(response.content, 'html.parser')\n",
    "   \n",
    "   # Find all links and extract URLs\n",
    "   links = soup.find_all('a', href=True)\n",
    "   urls = [link['href'] for link in links]\n",
    "   \n",
    "   return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e75e21e-6781-4424-9f49-74d0cf45991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_taxi_parquet_urls(all_urls: list[str]) -> list[str]:\n",
    "   r\"\"\"\n",
    "   Find URLs of yellow taxi parquet files from a list of URLs.\n",
    "   \n",
    "   Matches parquet files for yellow taxis from 2020-2024 with valid month numbers.\n",
    "\n",
    "   Args:\n",
    "       all_urls: List of URLs to search through\n",
    "\n",
    "   Returns:\n",
    "       list[str]: List of matched yellow taxi parquet file URLs\n",
    "\n",
    "   Note:\n",
    "       Expected to find 57 parquet files for the period 2020-2024\n",
    "   \"\"\"\n",
    "   # Define regex pattern for yellow taxi parquet files (2020-2024)\n",
    "   yellow_taxi_pattern = re.compile(\n",
    "       r'.*yellow_trip[-]?data_202[0-4]-(0[1-9]|1[0-2])\\.parquet$',\n",
    "       re.IGNORECASE\n",
    "   )\n",
    "   \n",
    "   # Filter URLs matching the pattern\n",
    "   yellow_taxi_links = [\n",
    "       url.strip() for url in all_urls \n",
    "       if yellow_taxi_pattern.match(url.strip())\n",
    "   ]\n",
    "   \n",
    "   # Log results\n",
    "   print(f\"Found {len(yellow_taxi_links)} yellow taxi parquet files\")  # should be 57\n",
    "   if yellow_taxi_links:\n",
    "       print(\"Sample URL:\", yellow_taxi_links[0])\n",
    "       \n",
    "   return yellow_taxi_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4ee2054-6f71-485b-9143-8cbe9f1569f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_uber_parquet_urls(all_urls: list[str]) -> list[str]:\n",
    "   r\"\"\"\n",
    "   Find URLs of Uber (FHVHV) parquet files from a list of URLs.\n",
    "   \n",
    "   Matches parquet files for Uber trips from 2020-2024 with valid month numbers.\n",
    "\n",
    "   Args:\n",
    "       all_urls: List of URLs to search through\n",
    "\n",
    "   Returns:\n",
    "       list[str]: List of matched Uber parquet file URLs\n",
    "\n",
    "   Note:\n",
    "       Expected to find 57 parquet files for the period 2020-2024\n",
    "   \"\"\"\n",
    "   # Define regex pattern for Uber parquet files (2020-2024)\n",
    "   uber_pattern = re.compile(\n",
    "       r'.*fhvhv_trip[-]?data_202[0-4]-(0[1-9]|1[0-2])\\.parquet$',\n",
    "       re.IGNORECASE\n",
    "   )\n",
    "   \n",
    "   # Filter URLs matching the pattern\n",
    "   uber_links = [\n",
    "       url.strip() for url in all_urls \n",
    "       if uber_pattern.match(url.strip())\n",
    "   ]\n",
    "   \n",
    "   # Log results\n",
    "   print(f\"Found {len(uber_links)} fhvhv parquet files\")  # should be 57\n",
    "   if uber_links:\n",
    "       print(\"Sample URL:\", uber_links[0])\n",
    "       \n",
    "   return uber_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50142af7-86d5-4f43-b7bf-166958a75d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_month(url: str) -> pd.DataFrame | None:\n",
    "   r\"\"\"\n",
    "   Download, sample, and clean monthly yellow taxi trip data.\n",
    "\n",
    "   Args:\n",
    "       url: URL or local path to parquet file containing taxi data\n",
    "\n",
    "   Returns:\n",
    "       pd.DataFrame: Cleaned and sampled taxi data with standardized columns\n",
    "       None: If processing fails\n",
    "\n",
    "   Notes:\n",
    "       - Downloads data if not cached locally\n",
    "       - Samples data using Cochran's formula\n",
    "       - Standardizes column names and formats\n",
    "       - Adds geolocation coordinates\n",
    "       - Identifies airport trips\n",
    "       - Fills missing values\n",
    "   \"\"\"\n",
    "   try:\n",
    "       # Load or download parquet file\n",
    "       filename = url.split('/')[-1]\n",
    "       if os.path.exists(f\"data/{filename}\"):\n",
    "           taxi_df = pd.read_parquet(f\"data/{filename}\")\n",
    "       else:\n",
    "           taxi_df = pd.read_parquet(url)\n",
    "           os.makedirs(\"data\", exist_ok=True)\n",
    "           taxi_df.to_parquet(f\"data/{filename}\")\n",
    "       \n",
    "       # Sample data\n",
    "       population_size = len(taxi_df)\n",
    "       sample_size = calculate_sample_size(population_size)\n",
    "       taxi_df = taxi_df.sample(n=sample_size, random_state=42)\n",
    "       \n",
    "       # Define required and optional columns\n",
    "       required_columns = ['tpep_pickup_datetime']\n",
    "       optional_columns = [\n",
    "           'trip_distance', 'extra', 'mta_tax', 'tip_amount', \n",
    "           'tolls_amount', 'improvement_surcharge', 'total_amount',\n",
    "           'congestion_surcharge', 'Airport_fee', 'PULocationID', \n",
    "           'DOLocationID', 'RatecodeID', 'tpep_dropoff_datetime'\n",
    "       ]\n",
    "       \n",
    "       # Validate and select columns\n",
    "       missing_columns = [col for col in required_columns if col not in taxi_df.columns]\n",
    "       if missing_columns:\n",
    "           raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "       \n",
    "       available_columns = required_columns + [col for col in optional_columns if col in taxi_df.columns]\n",
    "       taxi_df = taxi_df[available_columns]\n",
    "       \n",
    "       # Add coordinates from taxi zones\n",
    "       loaded_taxi_zones = load_taxi_zones()\n",
    "       taxi_df['pickup_coords'] = taxi_df['PULocationID'].apply(\n",
    "           lambda loc_id: lookup_coords_for_taxi_zone_id(loc_id, loaded_taxi_zones)\n",
    "       )\n",
    "       taxi_df['dropoff_coords'] = taxi_df['DOLocationID'].apply(\n",
    "           lambda loc_id: lookup_coords_for_taxi_zone_id(loc_id, loaded_taxi_zones)\n",
    "       )\n",
    "       taxi_df = taxi_df.dropna(subset=['pickup_coords', 'dropoff_coords'])\n",
    "       \n",
    "       # Process datetime fields\n",
    "       taxi_df['tpep_pickup_datetime'] = pd.to_datetime(taxi_df['tpep_pickup_datetime'])\n",
    "       taxi_df['tpep_dropoff_datetime'] = pd.to_datetime(taxi_df['tpep_dropoff_datetime'])\n",
    "       taxi_df[\"weekday_num\"] = taxi_df[\"tpep_dropoff_datetime\"].dt.weekday + 1\n",
    "       \n",
    "       # Calculate total amount if missing\n",
    "       taxi_df['total_amount'] = taxi_df.apply(\n",
    "           lambda row: (\n",
    "               row['extra'] + row['fare_amount'] + row['mta_tax'] + \n",
    "               row['airport_fee'] + row['Improvement_surcharge'] + \n",
    "               row['tolls_amount'] + row['congestion_surcharge']\n",
    "           ) if pd.isna(row['total_amount']) and \n",
    "                row[['fare_amount']].notna().all()\n",
    "           else row['total_amount'],\n",
    "           axis=1\n",
    "       )\n",
    "       \n",
    "       # Identify airport trips\n",
    "       taxi_df['airport'] = 'not airport'\n",
    "       taxi_df.loc[taxi_df['RatecodeID'] == 2, 'airport'] = 'JFK'\n",
    "       taxi_df.loc[taxi_df['RatecodeID'] == 3, 'airport'] = 'EWR'\n",
    "       taxi_df.loc[\n",
    "           (taxi_df['Airport_fee'] == 1.75) & (taxi_df['RatecodeID'] != 2),\n",
    "           'airport'\n",
    "       ] = 'LGA'\n",
    "\n",
    "       # Clean and standardize\n",
    "       taxi_df = taxi_df.drop(columns=['PULocationID', 'DOLocationID'])\n",
    "       taxi_df = taxi_df.rename(columns={\n",
    "           'tpep_pickup_datetime': 'pickup_datetime', \n",
    "           'tpep_dropoff_datetime': 'dropoff_datetime'\n",
    "       })\n",
    "       \n",
    "       # Fill missing values\n",
    "       columns_to_fill = [\n",
    "           'trip_distance', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', \n",
    "           'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'Airport_fee'\n",
    "       ]\n",
    "       taxi_df[columns_to_fill] = taxi_df[columns_to_fill].fillna(0)\n",
    "       \n",
    "       # Format coordinates\n",
    "       taxi_df['pickup_coords'] = taxi_df['pickup_coords'].apply(lambda x: f\"{x[0]},{x[1]}\")\n",
    "       taxi_df['dropoff_coords'] = taxi_df['dropoff_coords'].apply(lambda x: f\"{x[0]},{x[1]}\")\n",
    "       \n",
    "       return taxi_df\n",
    "       \n",
    "   except Exception as e:\n",
    "       print(f\"Error processing {url}: {e}\")\n",
    "       return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls):\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    for parquet_url in parquet_urls:\n",
    "        taxi_df = get_and_clean_taxi_month(parquet_url)\n",
    "        if taxi_df is not None:\n",
    "            all_taxi_dataframes.append(taxi_df)\n",
    "    \n",
    "    if not all_taxi_dataframes:\n",
    "        raise ValueError(\"No valid taxi data found\")\n",
    "        \n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "200776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data():\n",
    "    all_urls = get_all_urls_from_tlc_page(TLC_URL)\n",
    "    all_parquet_urls = find_taxi_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_taxi_data(all_parquet_urls)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "617bfdd7-8a09-4c07-a2c5-661a90913548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 57 yellow taxi parquet files\n",
      "Sample URL: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-04.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-05.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-06.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-07.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-08.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-09.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-04.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-05.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-06.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-07.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-08.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-09.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-10.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-11.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-12.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-02.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-03.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-04.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-05.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-06.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-07.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-08.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-09.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-10.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-11.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-12.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-02.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-03.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-04.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-05.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-06.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-07.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-08.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-09.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-10.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-11.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-12.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-01.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-02.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-03.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-04.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-05.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-06.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-07.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-08.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-09.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-10.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-11.parquet: Could not load taxi zones shapefile\n",
      "Error loading shapefile: taxi_zones.shp: No such file or directory\n",
      "Error processing https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-12.parquet: Could not load taxi zones shapefile\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No valid taxi data found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m taxi_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_taxi_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m, in \u001b[0;36mget_taxi_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m all_urls \u001b[38;5;241m=\u001b[39m get_all_urls_from_tlc_page(TLC_URL)\n\u001b[1;32m      3\u001b[0m all_parquet_urls \u001b[38;5;241m=\u001b[39m find_taxi_parquet_urls(all_urls)\n\u001b[0;32m----> 4\u001b[0m taxi_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_and_clean_taxi_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_parquet_urls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m taxi_data\n",
      "Cell \u001b[0;32mIn[29], line 10\u001b[0m, in \u001b[0;36mget_and_clean_taxi_data\u001b[0;34m(parquet_urls)\u001b[0m\n\u001b[1;32m      7\u001b[0m         all_taxi_dataframes\u001b[38;5;241m.\u001b[39mappend(taxi_df)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_taxi_dataframes:\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid taxi data found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m taxi_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(all_taxi_dataframes)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m taxi_data\n",
      "\u001b[0;31mValueError\u001b[0m: No valid taxi data found"
     ]
    }
   ],
   "source": [
    "taxi_data = get_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71f68cd-10b5-425b-a5cc-b43141ef41db",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81867c95-fe01-49f6-8aec-3b8fb736a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c7f5e0-ef5e-484a-a32c-0e2f02875d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae914786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_month(url: str) -> pd.DataFrame | None:\n",
    "   r\"\"\"\n",
    "   Download, sample and clean monthly Uber (FHVHV) trip data.\n",
    "\n",
    "   Args:\n",
    "       url: URL or local path to parquet file containing Uber data\n",
    "\n",
    "   Returns:\n",
    "       pd.DataFrame: Cleaned and sampled Uber data with standardized columns \n",
    "       None: If processing fails\n",
    "\n",
    "   Notes:\n",
    "       - Downloads data if not cached locally\n",
    "       - Samples data using Cochran's formula\n",
    "       - Filters for Uber trips only (HV0003)\n",
    "       - Adds geolocation coordinates and airport identification\n",
    "       - Standardizes column names and formats\n",
    "       - Fills missing values\n",
    "   \"\"\"\n",
    "   try:\n",
    "       # Load or download parquet file\n",
    "       filename = url.split('/')[-1]\n",
    "       if os.path.exists(f\"data/{filename}\"):\n",
    "           uber_df = pd.read_parquet(f\"data/{filename}\")\n",
    "       else:\n",
    "           uber_df = pd.read_parquet(url)\n",
    "           os.makedirs(\"data\", exist_ok=True)\n",
    "           uber_df.to_parquet(f\"data/{filename}\")\n",
    "\n",
    "       # Sample data\n",
    "       population_size = len(uber_df)\n",
    "       sample_size = calculate_sample_size(population_size)\n",
    "       uber_df = uber_df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "       # Define required and optional columns\n",
    "       required_columns = [\n",
    "           'hvfhs_license_num', 'pickup_datetime'\n",
    "       ]\n",
    "       optional_columns = [\n",
    "           'trip_miles', 'base_passenger_fare', 'tolls', 'sales_tax', \n",
    "           'congestion_surcharge', 'airport_fee', 'driver_pay', 'bcf',\n",
    "           'PULocationID', 'DOLocationID', 'dropoff_datetime', 'tips'\n",
    "       ]\n",
    "\n",
    "       # Validate columns\n",
    "       if not all(col in uber_df.columns for col in required_columns):\n",
    "           raise ValueError(\n",
    "               f\"Missing required columns: {[col for col in required_columns if col not in uber_df.columns]}\"\n",
    "           )\n",
    "\n",
    "       # Select columns and filter for Uber trips\n",
    "       available_columns = required_columns + [col for col in optional_columns if col in uber_df.columns]\n",
    "       uber_df = uber_df[available_columns]\n",
    "       uber_df = uber_df[uber_df['hvfhs_license_num'] == 'HV0003']\n",
    "\n",
    "       # Add coordinates from taxi zones\n",
    "       loaded_taxi_zones = load_taxi_zones()\n",
    "       uber_df['pickup_coords'] = uber_df['PULocationID'].apply(\n",
    "           lambda loc_id: lookup_coords_for_taxi_zone_id(loc_id, loaded_taxi_zones)\n",
    "       )\n",
    "       uber_df['dropoff_coords'] = uber_df['DOLocationID'].apply(\n",
    "           lambda loc_id: lookup_coords_for_taxi_zone_id(loc_id, loaded_taxi_zones)\n",
    "       )\n",
    "       uber_df = uber_df.dropna(subset=['pickup_coords', 'dropoff_coords'])\n",
    "\n",
    "       # Process datetime fields\n",
    "       uber_df['pickup_datetime'] = pd.to_datetime(uber_df['pickup_datetime'])\n",
    "       uber_df['dropoff_datetime'] = pd.to_datetime(uber_df['dropoff_datetime']) \n",
    "       uber_df[\"weekday_num\"] = uber_df[\"dropoff_datetime\"].dt.weekday + 1\n",
    "\n",
    "       # Calculate total amount\n",
    "       uber_df['total_amount'] = uber_df.apply(\n",
    "           lambda row: (\n",
    "               row['base_passenger_fare'] + row['tolls'] + row['sales_tax'] +\n",
    "               row['airport_fee'] + row['congestion_surcharge'] +\n",
    "               row['driver_pay'] + row['bcf']\n",
    "           ),\n",
    "           axis=1\n",
    "       )\n",
    "\n",
    "       # Define haversine distance calculation\n",
    "       def haversine(lat1: float, lon1: float, lat2: float, lon2: float) -> float:\n",
    "           \"\"\"Calculate haversine distance between two points.\"\"\"\n",
    "           R = 6371\n",
    "           lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "           dlat = lat2 - lat1\n",
    "           dlon = lon2 - lon1\n",
    "           a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "           c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "           return R * c\n",
    "\n",
    "       # Define airport locations\n",
    "       airports = {\n",
    "           \"JFK\": {\"lat\": 40.6413, \"lon\": -73.7781, \"radius\": 5},\n",
    "           \"LGA\": {\"lat\": 40.7769, \"lon\": -73.8740, \"radius\": 5},\n",
    "           \"EWR\": {\"lat\": 40.6895, \"lon\": -74.1745, \"radius\": 5}\n",
    "       }\n",
    "\n",
    "       def assign_airport(row: pd.Series) -> str:\n",
    "           \"\"\"Determine if trip starts or ends at an airport.\"\"\"\n",
    "           pickup_coords = row['pickup_coords']\n",
    "           dropoff_coords = row['dropoff_coords']\n",
    "           pickup_lat, pickup_lon = pickup_coords\n",
    "           dropoff_lat, dropoff_lon = dropoff_coords\n",
    "\n",
    "           for airport, info in airports.items():\n",
    "               pickup_distance = haversine(pickup_lat, pickup_lon, info['lat'], info['lon'])\n",
    "               if pickup_distance <= info['radius']:\n",
    "                   return airport\n",
    "\n",
    "               dropoff_distance = haversine(dropoff_lat, dropoff_lon, info['lat'], info['lon'])\n",
    "               if dropoff_distance <= info['radius']:\n",
    "                   return airport\n",
    "\n",
    "           return \"not airport\"\n",
    "\n",
    "       # Identify airport trips\n",
    "       uber_df['airport'] = uber_df.apply(assign_airport, axis=1)\n",
    "\n",
    "       # Clean and standardize\n",
    "       uber_df = uber_df.drop(columns=['PULocationID', 'DOLocationID'])\n",
    "       columns_to_fill = [\n",
    "           'trip_miles', 'base_passenger_fare', 'tolls', 'sales_tax',\n",
    "           'congestion_surcharge', 'airport_fee', 'driver_pay', 'bcf'\n",
    "       ]\n",
    "       uber_df[columns_to_fill] = uber_df[columns_to_fill].fillna(0)\n",
    "       \n",
    "       # Format coordinates as strings\n",
    "       uber_df['pickup_coords'] = uber_df['pickup_coords'].apply(lambda x: f\"{x[0]},{x[1]}\")\n",
    "       uber_df['dropoff_coords'] = uber_df['dropoff_coords'].apply(lambda x: f\"{x[0]},{x[1]}\")\n",
    "\n",
    "       return uber_df\n",
    "\n",
    "   except Exception as e:\n",
    "       print(f\"Error processing {url}: {e}\")\n",
    "       return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3d85ff-313c-41a2-9a46-261a9a2bb10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_data(parquet_urls):\n",
    "    all_uber_dataframes = []\n",
    "    \n",
    "    for parquet_url in parquet_urls:\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_uber_month(parquet_url)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_uber_dataframes.append(dataframe)\n",
    "\n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    uber_data = pd.concat(all_uber_dataframes)\n",
    "    return uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    all_urls = get_all_urls_from_tlc_page(TLC_URL)\n",
    "    all_parquet_urls = find_uber_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_uber_data(all_parquet_urls)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59291351-8ca7-4e15-abe7-e610c0700fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc495b9-9dcd-4b74-9f98-23efc437eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fd482e-6045-440c-90c6-a41b7153d3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory: str | None = None) -> list[str]:\n",
    "   r\"\"\"\n",
    "   Get URLs for weather data CSV files from 2020-2024.\n",
    "\n",
    "   Args:\n",
    "       directory: Optional local directory path (not used currently)\n",
    "\n",
    "   Returns:\n",
    "       list[str]: List of URLs for weather data CSV files\n",
    "\n",
    "   Notes:\n",
    "       Currently returns hardcoded GitHub URLs for 2020-2024 weather data\n",
    "   \"\"\"\n",
    "   weather_urls = [\n",
    "       \"https://raw.githubusercontent.com/Joanna-Wu-Weijia/4501-Final-Project/refs/heads/main/weather%20data/2020_weather.csv\",\n",
    "       \"https://raw.githubusercontent.com/Joanna-Wu-Weijia/4501-Final-Project/refs/heads/main/weather%20data/2021_weather.csv\", \n",
    "       \"https://raw.githubusercontent.com/Joanna-Wu-Weijia/4501-Final-Project/refs/heads/main/weather%20data/2022_weather.csv\",\n",
    "       \"https://raw.githubusercontent.com/Joanna-Wu-Weijia/4501-Final-Project/refs/heads/main/weather%20data/2023_weather.csv\",\n",
    "       \"https://raw.githubusercontent.com/Joanna-Wu-Weijia/4501-Final-Project/refs/heads/main/weather%20data/2024_weather.csv\",\n",
    "   ]\n",
    "   return weather_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866a3e96-e485-477c-9a3a-d14a94fc2c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file: str) -> pd.DataFrame:\n",
    "   r\"\"\"\n",
    "   Clean and process hourly weather data from CSV file.\n",
    "\n",
    "   Args:\n",
    "       csv_file: Path to weather data CSV file\n",
    "\n",
    "   Returns:\n",
    "       pd.DataFrame: Cleaned weather data with standardized columns and formats\n",
    "\n",
    "   Notes:\n",
    "       - Standardizes column names\n",
    "       - Maps weather type codes to human-readable labels\n",
    "       - Converts data types and handles missing values\n",
    "       - Adds derived columns for hour, weekday, and severe weather\n",
    "   \"\"\"\n",
    "   # Load and select columns\n",
    "   weather_data = pd.read_csv(csv_file)\n",
    "   weather_data = weather_data[[\n",
    "       'DATE', 'HourlyPresentWeatherType', 'HourlyDryBulbTemperature',\n",
    "       'HourlyPrecipitation', 'HourlyWindSpeed'\n",
    "   ]]\n",
    "\n",
    "   # Standardize column names\n",
    "   weather_data = weather_data.rename(columns={\n",
    "       'DATE': 'date',\n",
    "       'HourlyPresentWeatherType': 'hourly weather type',\n",
    "       'HourlyDryBulbTemperature': 'hourly temperature',\n",
    "       'HourlyPrecipitation': 'hourly precipitation',\n",
    "       'HourlyWindSpeed': 'hourly windspeed'\n",
    "   })\n",
    "\n",
    "   # Convert numeric columns\n",
    "   numeric_columns = ['hourly temperature', 'hourly precipitation', 'hourly windspeed']\n",
    "   for col in numeric_columns:\n",
    "       weather_data[col] = pd.to_numeric(weather_data[col], errors='coerce')\n",
    "\n",
    "   # Define weather type mapping\n",
    "   weather_mapping = {\n",
    "       '-RA:02 |RA |RA': 'rain',\n",
    "       '-RA:02 BR:1 |RA |RA': 'rain/mist',\n",
    "       'BR:1 ||': 'mist',\n",
    "       'HZ:7 |FU |HZ': 'haze/smoke',\n",
    "       'RA:02 BR:1 |RA |RA': 'rain/mist',\n",
    "       'FG:2 |FG |': 'fog',\n",
    "       '-SN:03 |SN |': 'snow',\n",
    "       '-SN:03 BR:1 |SN |': 'snow/mist',\n",
    "       '+RA:02 |RA |RA': 'rain',\n",
    "       '|SN |': 'snow',\n",
    "       '+SN:03 |SN s |': 'heavy snow',\n",
    "       '+SN:03 FZ:8 FG:2 |FG SN |': 'snow/frezzing/fog',\n",
    "       '-SN:03 FZ:8 FG:2 |FG SN |': 'snow/frezzing/fog',\n",
    "       'SN:03 FZ:8 FG:2 |FG SN |': 'snow/frezzing/fog',\n",
    "       '+RA:02 FG:2 |FG RA |RA': 'rain/fog',\n",
    "       'HZ:7 ||HZ': 'haze',\n",
    "       '|RA |': 'rain',\n",
    "       'RA:02 |RA |RA': 'rain',\n",
    "       'UP:09 ||': 'unknown',\n",
    "       'UP:09 BR:1 ||': 'mist',\n",
    "       '+RA:02 BR:1 |RA |RA': 'rain',\n",
    "       '-RA:02 ||': 'rain',\n",
    "       'RA:02 FG:2 |FG RA |RA': 'rain/fog',\n",
    "       '-RA:02 FG:2 |FG RA |RA': 'rain/fog',\n",
    "       'SN:03 |SN s |s': 'snow',\n",
    "       '-SN:03 FG:2 |FG SN |': 'snow/fog',\n",
    "       'SN:03 FG:2 |FG SN |': 'snow/fog'\n",
    "   }\n",
    "\n",
    "   # Map weather types and handle special cases\n",
    "   weather_data['hourly weather type'] = weather_data['hourly weather type'].map(weather_mapping)\n",
    "   weather_data.loc[weather_data['hourly precipitation'] == 0, 'hourly weather type'] = 'sunny'\n",
    "   weather_data.loc[\n",
    "       (weather_data['hourly precipitation'].isna()) & \n",
    "       (weather_data['hourly weather type'].isna()),\n",
    "       'hourly weather type'\n",
    "   ] = 'unknown'\n",
    "\n",
    "   # Process datetime and add derived columns\n",
    "   weather_data['date'] = pd.to_datetime(weather_data['date'])\n",
    "   weather_data['hour'] = weather_data['date'].dt.hour\n",
    "   weather_data['weekday_num'] = weather_data['date'].dt.weekday\n",
    "\n",
    "   # Define and mark severe weather conditions\n",
    "   severe_weather_conditions = [\n",
    "       '-SN:03 |SN |', '-SN:03 BR:1 |SN |', '+RA:02 |RA |RA', '|SN |',\n",
    "       '+SN:03 |SN s |', '+SN:03 FZ:8 FG:2 |FG SN |', '-SN:03 FZ:8 FG:2 |FG SN |',\n",
    "       'SN:03 FZ:8 FG:2 |FG SN |', '+RA:02 FG:2 |FG RA |RA', '+RA:02 BR:1 |RA |RA',\n",
    "       'SN:03 |SN s |s', '-SN:03 FG:2 |FG SN |', 'SN:03 FG:2 |FG SN |'\n",
    "   ]\n",
    "\n",
    "   weather_data['severe weather'] = 0\n",
    "   weather_data.loc[weather_data['hourly weather type'].isin(\n",
    "       [weather_mapping[condition] for condition in severe_weather_conditions]\n",
    "   ), 'severe weather'] = 1\n",
    "   weather_data.loc[weather_data['hourly weather type'] == 'unknown', 'severe weather'] = None\n",
    "\n",
    "   return weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48889a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file: str) -> pd.DataFrame:\n",
    "   r\"\"\"\n",
    "   Clean and aggregate hourly weather data to daily summaries.\n",
    "\n",
    "   Args:\n",
    "       csv_file: Path to weather data CSV file\n",
    "\n",
    "   Returns:\n",
    "       pd.DataFrame: Daily aggregated weather data with columns:\n",
    "           - date: Date of weather records\n",
    "           - daily weather type: Dominant weather type for the day\n",
    "           - daily temperature: Average temperature\n",
    "           - daily precipitation: Average precipitation\n",
    "           - daily windspeed: Average wind speed\n",
    "\n",
    "   Notes:\n",
    "       - Aggregates hourly data to daily summaries\n",
    "       - Determines daily weather type based on priority (snow > rain > other)\n",
    "       - Averages numeric measurements (temperature, precipitation, wind speed)\n",
    "   \"\"\"\n",
    "   # Load and select columns\n",
    "   weather_data = pd.read_csv(csv_file)\n",
    "   weather_data = weather_data[[\n",
    "       'DATE', 'HourlyPresentWeatherType', 'HourlyDryBulbTemperature',\n",
    "       'HourlyPrecipitation', 'HourlyWindSpeed'\n",
    "   ]]\n",
    "   \n",
    "   # Standardize column names\n",
    "   weather_data.columns = [\n",
    "       'datetime', 'weather_type', 'temperature', \n",
    "       'precipitation', 'wind_speed'\n",
    "   ]\n",
    "\n",
    "   # Convert data types\n",
    "   weather_data['date'] = pd.to_datetime(weather_data['datetime']).dt.date\n",
    "   numeric_columns = ['temperature', 'precipitation', 'wind_speed']\n",
    "   for col in numeric_columns:\n",
    "       weather_data[col] = pd.to_numeric(weather_data[col], errors='coerce')\n",
    "\n",
    "   # Define weather type mapping\n",
    "   weather_mapping = {\n",
    "       # Rain conditions\n",
    "       '-RA:02 |RA |RA': 'rain',\n",
    "       '-RA:02 BR:1 |RA |RA': 'rain', \n",
    "       'RA:02 BR:1 |RA |RA': 'rain',\n",
    "       '+RA:02 |RA |RA': 'rain',\n",
    "       '+RA:02 FG:2 |FG RA |RA': 'rain',\n",
    "       '|RA |': 'rain',\n",
    "       'RA:02 |RA |RA': 'rain',\n",
    "       '+RA:02 BR:1 |RA |RA': 'rain',\n",
    "       '-RA:02 ||': 'rain',\n",
    "       'RA:02 FG:2 |FG RA |RA': 'rain',\n",
    "       '-RA:02 FG:2 |FG RA |RA': 'rain',\n",
    "       \n",
    "       # Snow conditions\n",
    "       '-SN:03 |SN |': 'snow',\n",
    "       '-SN:03 BR:1 |SN |': 'snow',\n",
    "       '|SN |': 'snow',\n",
    "       '+SN:03 |SN s |': 'snow',\n",
    "       '+SN:03 FZ:8 FG:2 |FG SN |': 'snow',\n",
    "       '-SN:03 FZ:8 FG:2 |FG SN |': 'snow',\n",
    "       'SN:03 FZ:8 FG:2 |FG SN |': 'snow',\n",
    "       'SN:03 |SN s |s': 'snow',\n",
    "       '-SN:03 FG:2 |FG SN |': 'snow',\n",
    "       'SN:03 FG:2 |FG SN |': 'snow',\n",
    "       \n",
    "       # Other conditions\n",
    "       'BR:1 ||': 'other',\n",
    "       'HZ:7 |FU |HZ': 'other',\n",
    "       'FG:2 |FG |': 'other',\n",
    "       'HZ:7 ||HZ': 'other',\n",
    "       'UP:09 ||': 'unknown',\n",
    "       'UP:09 BR:1 ||': 'other'\n",
    "   }\n",
    "\n",
    "   # Map weather types\n",
    "   weather_data['MappedWeather'] = weather_data['weather_type'].map(weather_mapping).fillna('other')\n",
    "\n",
    "   def determine_weather_type(weather_series: pd.Series) -> str:\n",
    "       \"\"\"Determine daily weather type with priority: snow > rain > other.\"\"\"\n",
    "       if 'snow' in weather_series.values:\n",
    "           return 'snow'\n",
    "       elif 'rain' in weather_series.values:\n",
    "           return 'rain'\n",
    "       return 'other'\n",
    "\n",
    "   # Aggregate to daily data\n",
    "   daily_aggregated = weather_data.groupby('date').agg({\n",
    "       'MappedWeather': determine_weather_type,\n",
    "       'temperature': 'mean',\n",
    "       'precipitation': 'mean',\n",
    "       'wind_speed': 'mean'\n",
    "   }).reset_index()\n",
    "\n",
    "   # Rename columns to final format\n",
    "   daily_aggregated.columns = [\n",
    "       'date', 'daily weather type', 'daily temperature',\n",
    "       'daily precipitation', 'daily windspeed'\n",
    "   ]\n",
    "\n",
    "   return daily_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data() -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "   r\"\"\"\n",
    "   Load and process weather data files into hourly and daily formats.\n",
    "   \n",
    "   Returns:\n",
    "       tuple: Two DataFrames containing:\n",
    "           - Hourly weather data concatenated from all input files\n",
    "           - Daily weather data concatenated from all input files\n",
    "           \n",
    "   Notes:\n",
    "       - Processes CSV files obtained from get_all_weather_csvs()\n",
    "       - Cleans and standardizes both hourly and daily formats\n",
    "       - Combines data from all years (2020-2024)\n",
    "   \"\"\"\n",
    "   # Get list of weather CSV files\n",
    "   weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "   \n",
    "   # Initialize lists to store processed DataFrames\n",
    "   hourly_dataframes = []\n",
    "   daily_dataframes = []\n",
    "\n",
    "   # Process each CSV file\n",
    "   for csv_file in weather_csv_files:\n",
    "       hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "       daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "       hourly_dataframes.append(hourly_dataframe)\n",
    "       daily_dataframes.append(daily_dataframe)\n",
    "       \n",
    "   # Combine all monthly data into single DataFrames\n",
    "   hourly_data = pd.concat(hourly_dataframes)\n",
    "   daily_data = pd.concat(daily_dataframes)\n",
    "   \n",
    "   return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48216557",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935261b7-ae23-427c-97ff-ea31aa4e44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dcb502-d1d1-447d-aa68-11bff0dc53b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090eb94-a5b0-4d93-bf82-a596d2521b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c074aa3-a5f2-4586-8748-411e1e6c11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather (\n",
    "   date TIMESTAMP NOT NULL,\n",
    "   hourly_weather_type VARCHAR(50),\n",
    "   hourly_temperature FLOAT,\n",
    "   hourly_precipitation FLOAT,\n",
    "   hourly_windspeed FLOAT,\n",
    "   hour INTEGER NOT NULL,\n",
    "   weekday_num INTEGER NOT NULL,\n",
    "   severe_weather FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather (\n",
    "   date DATE NOT NULL,\n",
    "   daily_weather_type VARCHAR(50),\n",
    "   avg_temperature FLOAT,\n",
    "   avg_precipitation FLOAT, \n",
    "   avg_windspeed FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips (\n",
    "   pickup_datetime TIMESTAMP NOT NULL,\n",
    "   dropoff_datetime TIMESTAMP NOT NULL,\n",
    "   rate_code_id FLOAT,\n",
    "   trip_distance FLOAT NOT NULL,\n",
    "   extra FLOAT,\n",
    "   mta_tax FLOAT,\n",
    "   tip_amount FLOAT,\n",
    "   tolls_amount FLOAT,\n",
    "   improvement_surcharge FLOAT,\n",
    "   total_amount FLOAT,\n",
    "   congestion_surcharge FLOAT,\n",
    "   airport_fee FLOAT,\n",
    "   pickup_coords VARCHAR(50),\n",
    "   dropoff_coords VARCHAR(50),\n",
    "   weekday_num INTEGER NOT NULL,\n",
    "   airport VARCHAR(50)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips (\n",
    "   hvfhs_license_num VARCHAR(50) NOT NULL,\n",
    "   pickup_datetime TIMESTAMP NOT NULL,\n",
    "   dropoff_datetime TIMESTAMP NOT NULL,\n",
    "   trip_miles FLOAT,\n",
    "   base_passenger_fare FLOAT,\n",
    "   tolls FLOAT,\n",
    "   sales_tax FLOAT,\n",
    "   congestion_surcharge FLOAT,\n",
    "   airport_fee FLOAT,\n",
    "   driver_pay FLOAT,\n",
    "   bcf FLOAT,\n",
    "   pickup_coords VARCHAR(50),\n",
    "   dropoff_coords VARCHAR(50),\n",
    "   weekday_num INTEGER NOT NULL,\n",
    "   total_amount FLOAT,\n",
    "   tips FLOAT,\n",
    "   airport VARCHAR(50)\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd0009-1a4b-4502-8ca4-c274f4b90178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict: dict[str, pd.DataFrame]) -> None:\n",
    "   r\"\"\"\n",
    "   Write multiple DataFrames to their corresponding database tables.\n",
    "\n",
    "   Args:\n",
    "       table_to_df_dict: Dictionary mapping table names to DataFrames\n",
    "\n",
    "   Notes:\n",
    "       - Creates tables if they don't exist using predefined schemas\n",
    "       - Supported tables:\n",
    "           - hourly_weather: Hourly weather measurements\n",
    "           - daily_weather: Daily weather summaries  \n",
    "           - taxi_trips: Yellow taxi trip records\n",
    "           - uber_trips: Uber trip records\n",
    "       - Standardizes column names before writing\n",
    "       - Appends data if table already exists\n",
    "   \"\"\"\n",
    "   # Create or verify tables exist\n",
    "   with engine.connect() as conn:\n",
    "       conn.execute(db.text(HOURLY_WEATHER_SCHEMA))\n",
    "       conn.execute(db.text(DAILY_WEATHER_SCHEMA))\n",
    "       conn.execute(db.text(TAXI_TRIPS_SCHEMA))\n",
    "       conn.execute(db.text(UBER_TRIPS_SCHEMA))\n",
    "       conn.commit()\n",
    "       \n",
    "       # Column mapping for each table\n",
    "       column_mappings = {\n",
    "           \"hourly_weather\": {\n",
    "               'date': 'date',\n",
    "               'hourly weather type': 'hourly_weather_type',\n",
    "               'hourly temperature': 'hourly_temperature',\n",
    "               'hourly precipitation': 'hourly_precipitation',\n",
    "               'hourly windspeed': 'hourly_windspeed',\n",
    "               'hour': 'hour',\n",
    "               'weekday_num': 'weekday_num',\n",
    "               'severe weather': 'severe_weather'\n",
    "           },\n",
    "           \"daily_weather\": {\n",
    "               'date': 'date',\n",
    "               'daily weather type': 'daily_weather_type',\n",
    "               'daily temperature': 'avg_temperature',\n",
    "               'daily precipitation': 'avg_precipitation',\n",
    "               'daily windspeed': 'avg_windspeed'\n",
    "           },\n",
    "           \"taxi_trips\": {\n",
    "               'pickup_datetime': 'pickup_datetime',\n",
    "               'dropoff_datetime': 'dropoff_datetime',\n",
    "               'RatecodeID': 'rate_code_id',\n",
    "               'trip_distance': 'trip_distance',\n",
    "               'extra': 'extra',\n",
    "               'mta_tax': 'mta_tax',\n",
    "               'tip_amount': 'tip_amount',\n",
    "               'tolls_amount': 'tolls_amount',\n",
    "               'improvement_surcharge': 'improvement_surcharge',\n",
    "               'total_amount': 'total_amount',\n",
    "               'congestion_surcharge': 'congestion_surcharge',\n",
    "               'Airport_fee': 'airport_fee',\n",
    "               'pickup_coords': 'pickup_coords',\n",
    "               'dropoff_coords': 'dropoff_coords',\n",
    "               'weekday_num': 'weekday_num',\n",
    "               'airport': 'airport'\n",
    "           },\n",
    "           \"uber_trips\": {\n",
    "               'hvfhs_license_num': 'hvfhs_license_num',\n",
    "               'pickup_datetime': 'pickup_datetime',\n",
    "               'dropoff_datetime': 'dropoff_datetime',\n",
    "               'trip_miles': 'trip_miles',\n",
    "               'base_passenger_fare': 'base_passenger_fare',\n",
    "               'tolls': 'tolls',\n",
    "               'sales_tax': 'sales_tax',\n",
    "               'congestion_surcharge': 'congestion_surcharge',\n",
    "               'airport_fee': 'airport_fee',\n",
    "               'driver_pay': 'driver_pay',\n",
    "               'bcf': 'bcf',\n",
    "               'pickup_coords': 'pickup_coords',\n",
    "               'dropoff_coords': 'dropoff_coords',\n",
    "               'weekday_num': 'weekday_num',\n",
    "               'total_amount': 'total_amount',\n",
    "               'airport': 'airport',\n",
    "               'tips': 'tips'\n",
    "           }\n",
    "       }\n",
    "       \n",
    "       # Write each DataFrame to its table\n",
    "       for table_name, df in table_to_df_dict.items():\n",
    "           # Standardize column names\n",
    "           if table_name in column_mappings:\n",
    "               df = df.rename(columns=column_mappings[table_name])\n",
    "           \n",
    "           # Write to database\n",
    "           df.to_sql(\n",
    "               name=table_name,\n",
    "               con=engine,\n",
    "               if_exists='append',\n",
    "               index=False\n",
    "           )\n",
    "           print(f\"Successfully wrote {len(df)} rows to table {table_name}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af37d20e-cd85-4afe-822c-5bbd46ad0304",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_weather_data,\n",
    "    \"daily_weather\": daily_weather_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    df = pd.read_sql(query, engine)\n",
    "    df.to_csv(outfile, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1 = \"\"\"\n",
    "WITH hourly_counts AS (\n",
    "    SELECT \n",
    "        CAST(strftime('%H', pickup_datetime) AS INTEGER) as X,\n",
    "        COUNT(*) as Y\n",
    "    FROM taxi_trips\n",
    "    WHERE \n",
    "        pickup_datetime >= '2020-01-01' \n",
    "        AND pickup_datetime < '2024-09-01'\n",
    "    GROUP BY strftime('%H', pickup_datetime)\n",
    ")\n",
    "SELECT \n",
    "    X,\n",
    "    Y,\n",
    "    ROUND(CAST(Y AS FLOAT) * 100 / (SELECT SUM(Y) FROM hourly_counts), 2) as percentage\n",
    "FROM hourly_counts\n",
    "ORDER BY X;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('project.db')\n",
    "hourly_stats = pd.read_sql_query(QUERY_1, conn)\n",
    "conn.close()\n",
    "hourly_stats.to_csv(\"hourly_taxi_popularity.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870487c8-7804-4528-94e4-705e9f911a17",
   "metadata": {},
   "source": [
    "### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67de921c-00de-48a6-98d2-15653965a62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2 = \"\"\"\n",
    "WITH daily_counts AS (\n",
    "    SELECT \n",
    "        weekday_num as X,\n",
    "        COUNT(*) as Y\n",
    "    FROM uber_trips\n",
    "    WHERE \n",
    "        pickup_datetime >= '2020-01-01' \n",
    "        AND pickup_datetime < '2024-09-01'\n",
    "    GROUP BY weekday_num\n",
    ")\n",
    "SELECT \n",
    "    X,\n",
    "    Y,\n",
    "    ROUND(CAST(Y AS FLOAT) * 100 / (SELECT SUM(Y) FROM daily_counts), 2) as percentage\n",
    "FROM daily_counts\n",
    "ORDER BY Y DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314e4e8c-b7df-4b1a-9e16-d4b4ff5c16f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('project.db')\n",
    "daily_stats = pd.read_sql_query(QUERY_2, conn)\n",
    "conn.close()\n",
    "daily_stats.to_csv(\"daily_uber_popularity.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d30a7d-415c-48f4-b861-c2d55e6ffd04",
   "metadata": {},
   "source": [
    "### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a214c2ac-033a-479c-992a-a74b762b7ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3 = \"\"\"\n",
    "WITH combined_trips AS (\n",
    "    SELECT trip_distance as distance\n",
    "    FROM taxi_trips\n",
    "    WHERE \n",
    "        pickup_datetime >= '2024-01-01' \n",
    "        AND pickup_datetime < '2024-02-01'\n",
    "    UNION ALL\n",
    "    SELECT trip_miles as distance\n",
    "    FROM uber_trips\n",
    "    WHERE \n",
    "        pickup_datetime >= '2024-01-01' \n",
    "        AND pickup_datetime < '2024-02-01'\n",
    "),\n",
    "sorted_distances AS (\n",
    "    SELECT \n",
    "        distance,\n",
    "        (ROW_NUMBER() OVER (ORDER BY distance) - 1.0) / \n",
    "        (COUNT(*) OVER () - 1.0) * 100 as percentile\n",
    "    FROM combined_trips\n",
    ")\n",
    "SELECT ROUND(distance, 2) as percentile_95\n",
    "FROM sorted_distances\n",
    "WHERE percentile >= 95\n",
    "ORDER BY distance ASC\n",
    "LIMIT 1;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0dc315-a540-4dd5-97e2-fbb9d3c33dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('project.db')\n",
    "daily_stats = pd.read_sql_query(QUERY_3, conn)\n",
    "conn.close()\n",
    "daily_stats.to_csv(\"ride_distance_percentile.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2579dd34-1479-4cc6-ae69-c5e146833b60",
   "metadata": {},
   "source": [
    "### Query 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ee6638-2ebf-4594-b522-e3becc23fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4 = \"\"\"\n",
    "WITH SnowDays AS (\n",
    "    SELECT \n",
    "        date AS snow_date,\n",
    "        avg_precipitation AS total_precipitation\n",
    "    FROM daily_weather\n",
    "    WHERE daily_weather_type = 'snow'\n",
    "),\n",
    "DailyRideCounts AS (\n",
    "    SELECT \n",
    "        DATE(pickup_datetime) AS ride_date,\n",
    "        COUNT(*) AS total_rides\n",
    "    FROM (\n",
    "        SELECT pickup_datetime FROM taxi_trips\n",
    "        UNION ALL\n",
    "        SELECT pickup_datetime FROM uber_trips\n",
    "    )\n",
    "    GROUP BY DATE(pickup_datetime)\n",
    ")\n",
    "SELECT \n",
    "    s.snow_date AS date,\n",
    "    s.total_precipitation,\n",
    "    COALESCE(d.total_rides, 0) AS total_rides\n",
    "FROM SnowDays s\n",
    "LEFT JOIN DailyRideCounts d\n",
    "ON s.snow_date = d.ride_date\n",
    "ORDER BY s.total_precipitation DESC\n",
    "LIMIT 10;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872536fa-4903-451c-bd00-540c83752db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('project.db')\n",
    "df = pd.read_sql_query(QUERY_4, conn)\n",
    "df.to_csv(\"buiest_trip.csv\", index=False)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532c5efd-f538-4c4c-9d81-8f899470ccf1",
   "metadata": {},
   "source": [
    "### Query 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29601382-5307-4aba-9a6c-08dd935641f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_5_FILENAME = \"snowiest_days_rides.csv\"\n",
    "QUERY_5 = \"\"\"\n",
    "WITH SnowDays AS (\n",
    "    SELECT \n",
    "        date AS snow_date,\n",
    "        avg_precipitation AS total_precipitation\n",
    "    FROM daily_weather\n",
    "    WHERE daily_weather_type = 'snow'\n",
    "),\n",
    "DailyRideCounts AS (\n",
    "    SELECT \n",
    "        DATE(pickup_datetime) AS ride_date,\n",
    "        COUNT(*) AS total_rides\n",
    "    FROM (\n",
    "        SELECT pickup_datetime FROM taxi_trips\n",
    "        UNION ALL\n",
    "        SELECT pickup_datetime FROM uber_trips\n",
    "    )\n",
    "    GROUP BY DATE(pickup_datetime)\n",
    ")\n",
    "SELECT \n",
    "    s.snow_date AS date,\n",
    "    s.total_precipitation,\n",
    "    COALESCE(d.total_rides, 0) AS total_rides\n",
    "FROM SnowDays s\n",
    "LEFT JOIN DailyRideCounts d\n",
    "ON s.snow_date = d.ride_date\n",
    "ORDER BY s.total_precipitation DESC\n",
    "LIMIT 10;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696c3e83-9964-43f4-b8dc-6514bcbab0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('project.db')\n",
    "df = pd.read_sql_query(QUERY_5, conn)\n",
    "df.to_csv(QUERY_5_FILENAME, index=False)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1089c2d-75a4-4cb2-8c3a-cccfe24c6076",
   "metadata": {},
   "source": [
    "### Query 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8bd073-f252-4bf3-b441-0ba992fdd7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection configuration\n",
    "conn = sqlite3.connect('project.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# SQL query to generate hourly data with weather and ride counts\n",
    "query = \"\"\"\n",
    "WITH RECURSIVE GeneratedHours AS (\n",
    "   SELECT datetime('2023-09-25 00:00:00') AS hour\n",
    "   UNION ALL\n",
    "   SELECT datetime(hour, '+1 hour') \n",
    "   FROM GeneratedHours\n",
    "   WHERE hour < '2023-10-03 23:00:00'\n",
    "),\n",
    "\n",
    "HourlyWeatherData AS (\n",
    "   SELECT \n",
    "       strftime('%Y-%m-%d %H:00:00', date || ' ' || hour || ':00:00') AS hour,\n",
    "       AVG(hourly_precipitation) AS precipitation,\n",
    "       AVG(hourly_windspeed) AS windspeed\n",
    "   FROM hourly_weather\n",
    "   WHERE date BETWEEN '2023-09-25' AND '2023-10-03'\n",
    "   GROUP BY strftime('%Y-%m-%d %H:00:00', date || ' ' || hour || ':00:00')\n",
    "),\n",
    "\n",
    "HourlyRideCounts AS (\n",
    "   SELECT \n",
    "       strftime('%Y-%m-%d %H:00:00', pickup_datetime) AS hour,\n",
    "       COUNT(*) AS total_rides\n",
    "   FROM (\n",
    "       SELECT pickup_datetime FROM taxi_trips\n",
    "       UNION ALL\n",
    "       SELECT pickup_datetime FROM uber_trips\n",
    "   )\n",
    "   WHERE pickup_datetime BETWEEN '2023-09-25 00:00:00' AND '2023-10-03 23:59:59'\n",
    "   GROUP BY strftime('%Y-%m-%d %H:00:00', pickup_datetime)\n",
    "),\n",
    "\n",
    "CombinedData AS (\n",
    "   SELECT \n",
    "       g.hour AS datetime,\n",
    "       COALESCE(r.total_rides, 0) AS total_rides,\n",
    "       COALESCE(w.precipitation, 0.0) AS precipitation,\n",
    "       COALESCE(w.windspeed, 0.0) AS windspeed\n",
    "   FROM GeneratedHours g\n",
    "   LEFT JOIN HourlyRideCounts r ON g.hour = r.hour\n",
    "   LEFT JOIN HourlyWeatherData w ON g.hour = w.hour\n",
    ")\n",
    "\n",
    "SELECT \n",
    "   datetime,\n",
    "   total_rides,\n",
    "   precipitation,\n",
    "   windspeed\n",
    "FROM CombinedData\n",
    "ORDER BY datetime ASC;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "   # Execute query and fetch results\n",
    "   cursor.execute(query)\n",
    "   result_tuples = cursor.fetchall()\n",
    "\n",
    "   # Display results\n",
    "   for row in result_tuples:\n",
    "       print(row)\n",
    "\n",
    "finally:\n",
    "   # Ensure database connection is properly closed\n",
    "   cursor.close()\n",
    "   conn.commit()\n",
    "   conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hourly_taxi_distribution(dataframe: pd.DataFrame) -> None:\n",
    "   r\"\"\"\n",
    "   Create a two-panel visualization of hourly taxi ride distribution.\n",
    "\n",
    "   Args:\n",
    "       dataframe: DataFrame containing columns:\n",
    "           - X: Hour of day (0-23)\n",
    "           - Y: Number of rides\n",
    "           - percentage: Percentage of total rides\n",
    "\n",
    "   Notes:\n",
    "       Creates two plots:\n",
    "       - Top: Absolute number of rides by hour\n",
    "       - Bottom: Percentage of total rides by hour\n",
    "       Both include grid lines and value labels above each bar\n",
    "   \"\"\"\n",
    "   # Set up figure with two subplots\n",
    "   figure, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "   # Plot absolute numbers\n",
    "   ax1.bar(dataframe['X'], dataframe['Y'], color='orange', alpha=0.7)\n",
    "   ax1.set_title('Hourly Distribution of Taxi Rides (2020-2024)', pad=20, size=14)\n",
    "   ax1.set_xlabel('Hour of Day')\n",
    "   ax1.set_ylabel('Number of Rides')\n",
    "   ax1.grid(True, alpha=0.3)\n",
    "   ax1.set_xticks(range(24))\n",
    "   \n",
    "   # Add value labels for absolute numbers\n",
    "   for i, v in enumerate(dataframe['Y']):\n",
    "       ax1.text(i, v, str(v), ha='center', va='bottom')\n",
    "   \n",
    "   # Plot percentages\n",
    "   ax2.bar(dataframe['X'], dataframe['percentage'], color='green', alpha=0.7)\n",
    "   ax2.set_title('Hourly Distribution of Taxi Rides (Percentage)', pad=20, size=14)\n",
    "   ax2.set_xlabel('Hour of Day')\n",
    "   ax2.set_ylabel('Percentage of Total Rides (%)')\n",
    "   ax2.grid(True, alpha=0.3)\n",
    "   ax2.set_xticks(range(24))\n",
    "   \n",
    "   # Add value labels for percentages\n",
    "   for i, v in enumerate(dataframe['percentage']):\n",
    "       ax2.text(i, v, f'{v:.2f}%', ha='center', va='bottom')\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "\n",
    "\n",
    "def get_hourly_taxi_data() -> pd.DataFrame:\n",
    "   r\"\"\"\n",
    "   Load hourly taxi ride data from CSV file.\n",
    "\n",
    "   Returns:\n",
    "       pd.DataFrame: Hourly taxi ride statistics\n",
    "   \"\"\"\n",
    "   return pd.read_csv('hourly_taxi_popularity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data_print = get_hourly_taxi_data()\n",
    "plot_hourly_taxi_distribution(taxi_data_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90983701-45cd-44c7-97cc-d72ddc3e0564",
   "metadata": {},
   "source": [
    "### Visualization 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a872593-f672-4633-8e1b-bcd1f46c8e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Use a separate assignment instead of inplace modification\n",
    "uber_data = uber_data.rename(columns={'trip_miles': 'trip_distance'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9d1027-3346-49e0-a2d6-5e4ee8794eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_trips = pd.concat([\n",
    "    taxi_data[['pickup_datetime', 'trip_distance']],\n",
    "    uber_data[['pickup_datetime', 'trip_distance']]\n",
    "])\n",
    "\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-08-31'\n",
    "\n",
    "combined_trips = combined_trips[\n",
    "    (combined_trips['pickup_datetime'] >= start_date) &\n",
    "    (combined_trips['pickup_datetime'] <= end_date)\n",
    "]\n",
    "\n",
    "combined_trips['month'] = combined_trips['pickup_datetime'].dt.month\n",
    "\n",
    "# Group by month and calculate average distance and confidence interval\n",
    "monthly_avg_distance = combined_trips.groupby('month')['trip_distance'].agg([\n",
    "    'mean',\n",
    "    'count',\n",
    "    'std'\n",
    "])\n",
    "\n",
    "# Z-value for 90% confidence interval\n",
    "z_value = 1.645\n",
    "\n",
    "# Calculate standard error of mean and confidence intervals\n",
    "monthly_avg_distance['sem'] = (\n",
    "    monthly_avg_distance['std'] / np.sqrt(monthly_avg_distance['count'])\n",
    ")\n",
    "monthly_avg_distance['ci'] = z_value * monthly_avg_distance['sem']\n",
    "\n",
    "# Reset index for plotting\n",
    "monthly_avg_distance.reset_index(inplace=True)\n",
    "\n",
    "# Convert data for plotting\n",
    "months = monthly_avg_distance['month'].values.astype(float)\n",
    "mean_values = monthly_avg_distance['mean'].values.astype(float)\n",
    "lower_bound = (mean_values - monthly_avg_distance['ci'].values).astype(float)\n",
    "upper_bound = (mean_values + monthly_avg_distance['ci'].values).astype(float)\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.lineplot(\n",
    "    x=months,\n",
    "    y=mean_values,\n",
    "    label='Average Distance',\n",
    "    color='blue'\n",
    ")\n",
    "\n",
    "plt.fill_between(\n",
    "    months,\n",
    "    lower_bound,\n",
    "    upper_bound,\n",
    "    color='b',\n",
    "    alpha=0.2,\n",
    "    label='90% Confidence Interval'\n",
    ")\n",
    "\n",
    "# Configure plot appearance\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Distance (miles)')\n",
    "plt.title(\n",
    "    'Average Distance Traveled per Month (January 2020 - August 2024)\\n'\n",
    "    'Taxis and Ubers Combined'\n",
    ")\n",
    "plt.xticks(\n",
    "    ticks=range(1, 13),\n",
    "    labels=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "            'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    ")\n",
    "plt.legend()\n",
    "plt.grid(visible=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cae62a-6be0-4e6b-9063-e91afb4cb691",
   "metadata": {},
   "source": [
    "### Visulization 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6593304-be07-438f-ba1b-2216fc47c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine trip data from taxi and uber sources\n",
    "combined_trips = pd.concat([\n",
    "    taxi_data[['pickup_datetime', 'trip_distance', 'dropoff_coords']],\n",
    "    uber_data[['pickup_datetime', 'trip_distance', 'dropoff_coords']]\n",
    "])\n",
    "\n",
    "# Define date range\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-08-31'\n",
    "\n",
    "# Filter trips within date range\n",
    "combined_trips = combined_trips[\n",
    "    (combined_trips['pickup_datetime'] >= start_date) &\n",
    "    (combined_trips['pickup_datetime'] <= end_date)\n",
    "]\n",
    "\n",
    "# Define airport bounding boxes as constants\n",
    "LGA_BOX_COORDS = (\n",
    "    (40.763589, -73.891745),\n",
    "    (40.778865, -73.854838)\n",
    ")\n",
    "JFK_BOX_COORDS = (\n",
    "    (40.639263, -73.795642),\n",
    "    (40.651376, -73.766264)\n",
    ")\n",
    "EWR_BOX_COORDS = (\n",
    "    (40.686794, -74.194028),\n",
    "    (40.699680, -74.165205)\n",
    ")\n",
    "\n",
    "\n",
    "def is_within_bbox(coord, bbox):\n",
    "    \"\"\"Check if coordinates fall within a bounding box.\"\"\"\n",
    "    lat, lon = coord\n",
    "    (lat_min, lon_min), (lat_max, lon_max) = bbox\n",
    "    return lat_min <= lat <= lat_max and lon_min <= lon <= lon_max\n",
    "\n",
    "\n",
    "def determine_airport(row):\n",
    "    \"\"\"Determine which airport a dropoff location corresponds to.\"\"\"\n",
    "    coords = eval(row['dropoff_coords'].replace('POINT(', '').replace(')', ''))\n",
    "    \n",
    "    if is_within_bbox(coords, LGA_BOX_COORDS):\n",
    "        return 'LGA'\n",
    "    elif is_within_bbox(coords, JFK_BOX_COORDS):\n",
    "        return 'JFK'\n",
    "    elif is_within_bbox(coords, EWR_BOX_COORDS):\n",
    "        return 'EWR'\n",
    "    return None\n",
    "\n",
    "\n",
    "# Add weekday and airport columns\n",
    "combined_trips['weekday'] = combined_trips['pickup_datetime'].dt.day_name()\n",
    "combined_trips['airport'] = combined_trips.apply(determine_airport, axis=1)\n",
    "\n",
    "# Filter and analyze airport trips\n",
    "airport_trips = combined_trips[combined_trips['airport'].isin(['LGA', 'JFK', 'EWR'])]\n",
    "airport_popularity = (\n",
    "    airport_trips.groupby(['airport', 'weekday'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    ")\n",
    "\n",
    "# Create pivot table for visualization\n",
    "weekday_order = [\n",
    "    'Monday', 'Tuesday', 'Wednesday', 'Thursday',\n",
    "    'Friday', 'Saturday', 'Sunday'\n",
    "]\n",
    "airport_popularity_pivot = airport_popularity.pivot(\n",
    "    index='weekday',\n",
    "    columns='airport',\n",
    "    values='count'\n",
    ").reindex(weekday_order)\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "airport_popularity_pivot.plot(kind='bar', figsize=(15, 12))\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Number of Drop-offs')\n",
    "plt.title('Most Popular Drop-off Days by Airport (January 2020 - August 2024)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Airport')\n",
    "plt.grid(visible=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc04655-ebb2-446c-a0e4-68e3dd8e426e",
   "metadata": {},
   "source": [
    "### Visulization 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3877f26e-8370-4de5-b2b0-6967290394f8",
   "metadata": {},
   "source": [
    "### Visulization 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9567e42f-931c-4bf8-ba65-754d76771e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round pickup times to nearest hour\n",
    "taxi_data['hour'] = taxi_data['pickup_datetime'].dt.round('H')\n",
    "uber_data['hour'] = uber_data['pickup_datetime'].dt.round('H')\n",
    "hourly_weather_data['hour'] = hourly_weather_data['date'].dt.round('H')\n",
    "\n",
    "# Merge weather data with trip data\n",
    "taxi_merged = pd.merge(\n",
    "   taxi_data,\n",
    "   hourly_weather_data[['hour', 'hourly precipitation']],\n",
    "   on='hour',\n",
    "   how='left'\n",
    ")\n",
    "uber_merged = pd.merge(\n",
    "   uber_data,\n",
    "   hourly_weather_data[['hour', 'hourly precipitation']],\n",
    "   on='hour',\n",
    "   how='left'\n",
    ")\n",
    "\n",
    "# Filter out invalid data points\n",
    "taxi_merged = taxi_merged[\n",
    "   (taxi_merged['trip_distance'] > 0) &\n",
    "   (taxi_merged['tip_amount'] > 0)\n",
    "]\n",
    "uber_merged = uber_merged[\n",
    "   (uber_merged['trip_distance'] > 0) &\n",
    "   (uber_merged['tips'] > 0)\n",
    "]\n",
    "\n",
    "# Remove outliers above 99th percentile\n",
    "taxi_merged = taxi_merged[\n",
    "   (taxi_merged['trip_distance'] < np.percentile(taxi_merged['trip_distance'], 99)) &\n",
    "   (taxi_merged['tip_amount'] < np.percentile(taxi_merged['tip_amount'], 99))\n",
    "]\n",
    "uber_merged = uber_merged[\n",
    "   (uber_merged['trip_distance'] < np.percentile(uber_merged['trip_distance'], 99)) &\n",
    "   (uber_merged['tips'] < np.percentile(uber_merged['tips'], 99))\n",
    "]\n",
    "\n",
    "# Create visualization grid\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot taxi tips vs distance\n",
    "axes[0, 0].scatter(\n",
    "   taxi_merged['trip_distance'],\n",
    "   taxi_merged['tip_amount'],\n",
    "   alpha=0.5,\n",
    "   color='blue'\n",
    ")\n",
    "axes[0, 0].set_xlabel('Distance (miles)')\n",
    "axes[0, 0].set_ylabel('Tip Amount ($)')\n",
    "axes[0, 0].set_title('Yellow Taxi Tips vs. Distance')\n",
    "\n",
    "# Plot Uber tips vs distance\n",
    "axes[0, 1].scatter(\n",
    "   uber_merged['trip_distance'],\n",
    "   uber_merged['tips'],\n",
    "   alpha=0.5,\n",
    "   color='red'\n",
    ")\n",
    "axes[0, 1].set_xlabel('Distance (miles)')\n",
    "axes[0, 1].set_ylabel('Tip Amount ($)')\n",
    "axes[0, 1].set_title('Uber Tips vs. Distance')\n",
    "\n",
    "# Plot taxi tips vs precipitation\n",
    "axes[1, 0].scatter(\n",
    "   taxi_merged['hourly precipitation'],\n",
    "   taxi_merged['tip_amount'],\n",
    "   alpha=0.5,\n",
    "   color='green'\n",
    ")\n",
    "axes[1, 0].set_xlabel('Hourly Precipitation (inches)')\n",
    "axes[1, 0].set_ylabel('Tip Amount ($)')\n",
    "axes[1, 0].set_title('Yellow Taxi Tips vs. Precipitation')\n",
    "\n",
    "# Plot Uber tips vs precipitation\n",
    "axes[1, 1].scatter(\n",
    "   uber_merged['hourly precipitation'],\n",
    "   uber_merged['tips'],\n",
    "   alpha=0.5,\n",
    "   color='purple'\n",
    ")\n",
    "axes[1, 1].set_xlabel('Hourly Precipitation (inches)')\n",
    "axes[1, 1].set_ylabel('Tip Amount ($)')\n",
    "axes[1, 1].set_title('Uber Tips vs. Precipitation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2665730-9266-4828-bd07-6db952d81557",
   "metadata": {},
   "source": [
    "### Visulization 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a61655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize map centered on NYC\n",
    "m = folium.Map(\n",
    "   location=[40.7128, -74.0060],\n",
    "   zoom_start=11\n",
    ")\n",
    "\n",
    "# Process taxi pickup coordinates\n",
    "taxi_coords = [\n",
    "   [\n",
    "       float(coord.split(',')[0]),\n",
    "       float(coord.split(',')[1])\n",
    "   ] \n",
    "   for coord in taxi_data['pickup_coords']\n",
    "]\n",
    "\n",
    "# Add taxi heatmap layer\n",
    "taxi_gradient = {\n",
    "   0.4: 'yellow',\n",
    "   0.65: 'orange',\n",
    "   1: 'red'\n",
    "}\n",
    "HeatMap(\n",
    "   taxi_coords,\n",
    "   radius=15,\n",
    "   gradient=taxi_gradient\n",
    ").add_to(m)\n",
    "\n",
    "# Process Uber pickup coordinates  \n",
    "uber_coords = [\n",
    "   [\n",
    "       float(coord.split(',')[0]), \n",
    "       float(coord.split(',')[1])\n",
    "   ]\n",
    "   for coord in uber_data['pickup_coords']\n",
    "]\n",
    "\n",
    "# Add Uber heatmap layer\n",
    "uber_gradient = {\n",
    "   0.4: 'blue',\n",
    "   0.65: 'purple',\n",
    "   1: 'red'\n",
    "}\n",
    "HeatMap(\n",
    "   uber_coords,\n",
    "   radius=15,\n",
    "   gradient=uber_gradient\n",
    ").add_to(m)\n",
    "\n",
    "# Save final map\n",
    "m.save('nyc_rides_heatmap_2020.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe31b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84f1cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
